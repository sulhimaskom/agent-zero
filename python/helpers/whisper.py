import base64
import warnings
import tempfile
import asyncio

# Whisper with graceful degradation
try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError as e:
    WHISPER_AVAILABLE = False
    whisper = None
    print(f"Warning: Whisper not available - audio transcription will be limited: {e}")

from python.helpers import runtime, rfc, settings, files
from python.helpers.print_style import PrintStyle
from python.helpers.notification import NotificationManager, NotificationType, NotificationPriority

# Suppress FutureWarning from torch.load
warnings.filterwarnings("ignore", category=FutureWarning)

_model = None
_model_name = ""
is_updating_model = False  # Tracks whether the model is currently updating

async def preload(model_name:str):
    if not WHISPER_AVAILABLE:
        PrintStyle.warning("Whisper is not available - cannot preload model")
        return None
    try:
        # return await runtime.call_development_function(_preload, model_name)
        return await _preload(model_name)
    except Exception as e:
        # if not runtime.is_development():
        raise e
        
async def _preload(model_name:str):
    if not WHISPER_AVAILABLE:
        return None
    global _model, _model_name, is_updating_model

    while is_updating_model:
        await asyncio.sleep(0.1)

    try:
        is_updating_model = True
        if not _model or _model_name != model_name:
            NotificationManager.send_notification(
                NotificationType.INFO,
                NotificationPriority.NORMAL,
                "Loading Whisper model...",
                display_time=99,
                group="whisper-preload")
            PrintStyle.standard(f"Loading Whisper model: {model_name}")
            _model = whisper.load_model(name=model_name, download_root=files.get_abs_path("/tmp/models/whisper")) # type: ignore
            _model_name = model_name
            NotificationManager.send_notification(
                NotificationType.INFO,
                NotificationPriority.NORMAL,
                "Whisper model loaded.",
                display_time=2,
                group="whisper-preload")
    finally:
        is_updating_model = False

async def is_downloading():
    # return await runtime.call_development_function(_is_downloading)
    return _is_downloading()

def _is_downloading():
    return is_updating_model

async def is_downloaded():
    try:
        # return await runtime.call_development_function(_is_downloaded)
        return _is_downloaded()
    except Exception as e:
        # if not runtime.is_development():
        raise e
        # Fallback to direct execution if RFC fails in development
        # return _is_downloaded()

def _is_downloaded():
    return _model is not None

async def transcribe(model_name:str, audio_bytes_b64: str):
    if not WHISPER_AVAILABLE:
        PrintStyle.warning("Whisper is not available - cannot transcribe audio")
        return {"text": "Whisper not available for audio transcription"}
    # return await runtime.call_development_function(_transcribe, model_name, audio_bytes_b64)
    return await _transcribe(model_name, audio_bytes_b64)


async def _transcribe(model_name:str, audio_bytes_b64: str):
    if not WHISPER_AVAILABLE or _model is None:
        return {"text": "Whisper not available for audio transcription"}
    await _preload(model_name)
    
    # Decode audio bytes if encoded as a base64 string
    audio_bytes = base64.b64decode(audio_bytes_b64)

    # Create temp audio file
    import os
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as audio_file:
        audio_file.write(audio_bytes)
        temp_path = audio_file.name
    try:
        # Transcribe the audio file
        result = _model.transcribe(temp_path, fp16=False) # type: ignore
        return result
    finally:
        try:
            os.remove(temp_path)
        except Exception:
            pass # ignore errors during cleanup
